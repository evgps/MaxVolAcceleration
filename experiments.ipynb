{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "import tables\n",
    "from types import SimpleNamespace\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "from torch.backends import cudnn \n",
    "#Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc, rcParams\n",
    "\n",
    "#Custom code\n",
    "from student import Student\n",
    "from feedforwardcompressed import FeedforwardCompressed, get_batch, slice_blocks, block_speed, timeit_speed\n",
    "from evaluator import Evaluator\n",
    "import dataloaders\n",
    "from randomized_svd import torch_randomized_svd\n",
    "from tools import plot_sigma, save_V_array, load_V_array, save_inv_ind\n",
    "from tools import load_inv_ind, compute_approx, HaveInterlayers, dump_svd, create_folders\n",
    "from models import vgg19, cifar10, cifar100, stl10\n",
    "from models.cifar import vgg19 as vgg19_100\n",
    "from train_validate import train, validate, adjust_learning_rate, save_checkpoint\n",
    "\n",
    "print('Should be > 1.0.0 : {}'.format(torch.__version__))\n",
    "cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = SimpleNamespace()\n",
    "config.dir_path = \n",
    "config.storage_path = \n",
    "config.data_root = \n",
    "\n",
    "config.device = 'cuda'\n",
    "config.cuda_device = 2\n",
    "config.batch_size = 128\n",
    "\n",
    "config.dataset = 'cifar10'\n",
    "config.model = 'vgg19'\n",
    "# vgg19, cifar10, cifar100, stl10\n",
    "\n",
    "#Steps of experiments:\n",
    "#For fast test one can use precomputer SVD's (check supported models):\n",
    "config.download_svd = False\n",
    "#Or calculate by themself (set download to false:\n",
    "config.calculate_interlayers = False\n",
    "config.compute_svd = False\n",
    "config.validate_svd = True\n",
    "config.plot_singular_values = True\n",
    "config.calc_compressed = True\n",
    "config.eval_compressed = True\n",
    "config.calc_finetune = True\n",
    "config.measure_speed = True\n",
    "config.vis_time_accuracy = True\n",
    "\n",
    "config.skip_if_computed = True\n",
    "\n",
    "#SVD step additional configuration\n",
    "config.svd_rank = 10000\n",
    "config.svd_maxval = 10000\n",
    "config.svd_nsamples = [80000]*20\n",
    "\n",
    "#Compression step additional configuration\n",
    "config.compression_block_num_list = [2,3,4,5,6]\n",
    "config.compression_rate = 0.1\n",
    "\n",
    "#Fine-tune additional configuration\n",
    "config.ft_variance = 0.5\n",
    "config.kd = False\n",
    "# - Adam's parameters\n",
    "optim_parameters = SimpleNamespace()\n",
    "optim_parameters.lr0 = 1e-4*0.5\n",
    "optim_parameters.weight_decay = 5e-4\n",
    "optim_parameters.momentum = 0.9\n",
    "optim_parameters.start_epoch = 0\n",
    "optim_parameters.epochs = 90\n",
    "# - KD loss's parameters (add loss from teacher)\n",
    "kd_params = None\n",
    "if config.kd:\n",
    "    kd_params = SimpleNamespace()\n",
    "    kd_params.alpha = 0.001\n",
    "    kd_params.temperature = 3.5\n",
    "    \n",
    "#Speed meter parameters:\n",
    "config.speed_device = 'cpu'\n",
    "config.speed_batchsize = 1\n",
    "print('Use config: {}'.format(config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(config.dir_path)\n",
    "os.environ['CUDA_DEVICE_ORDER']='PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES']=str(config.cuda_device)\n",
    "\n",
    "create_folders(config)\n",
    "#Create dataloaders\n",
    "loaders = dataloaders.get_loader(config.batch_size, config.dataset, data_root=config.data_root,\n",
    "                                     num_workers=4, drop_last=True)\n",
    "\n",
    "#Load pretrained model\n",
    "print('Loading pretrained initial model {}\\n'.format(config.model))\n",
    "if config.model == 'vgg19' and config.dataset == 'cifar100':\n",
    "    name = config.dataset+'/'+config.model\n",
    "    if not os.path.exists(config.storage_path+'pretrained/'+config.dataset+'/'\n",
    "                    +config.model+'/'+'model_best.pth.tar'):\n",
    "        print('please, pu')\n",
    "    model = vgg19_100(num_classes = 100,\n",
    "                     pretrained = config.storage_path+'pretrained/'+config.dataset+'/'\n",
    "                    +config.model+'/'+'model_best.pth.tar')\n",
    "    \n",
    "if config.model == 'vgg19' and config.dataset == 'cifar10':\n",
    "    name = config.dataset+'/'+config.model\n",
    "    if not os.path.exists(config.storage_path+'pretrained/'+config.dataset+'/'\n",
    "                    +config.model+'/'+'model_best.pth.tar'):\n",
    "        print('please, pu')\n",
    "    model = vgg19(pretrained = config.storage_path+'pretrained/'+config.dataset+'/'\n",
    "                    +config.model+'/'+'model_best.pth.tar')\n",
    "\n",
    "if config.model == 'playground' and config.dataset == 'cifar10':\n",
    "    if not os.path.exists(config.storage_path+'pretrained/'+config.dataset+'/'\n",
    "                    +config.model+'/'+'model_best.pth.tar'):\n",
    "        print('please, pu')\n",
    "    model = cifar10(n_channel=128, pretrained = True)\n",
    "\n",
    "elif config.model == 'svhn':\n",
    "    name = ['svhn/']        \n",
    "    model = [(models.svhn(32, pretrained=True))]\n",
    "    \n",
    "#Create compression list for choosen model:\n",
    "tmp, _ = slice_blocks(model, loaders['val'], config.device)\n",
    "n = len(tmp)\n",
    "config.compression_rate_list = []\n",
    "config.teacher=[None]*n\n",
    "config.compression_rate_list.append(config.teacher)\n",
    "for k in config.compression_block_num_list: \n",
    "    compression_rate =  [None]*(n - k) + [config.compression_rate] * k \n",
    "    config.compression_rate_list.append(compression_rate)      \n",
    "config.finetune_compression_rate_list = config.compression_rate_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not config.download_svd:\n",
    "    #You need a lot of RAM on two following steps, so, you also can download SVD\n",
    "    # Interlayer outputs are needed for SVD\n",
    "    if config.calculate_interlayers:\n",
    "        print('Calculating and store outputs after each block in model ...\\n')    \n",
    "\n",
    "        tmp_path = config.storage_path+'interlayers/'+config.dataset+'/'+config.model\n",
    "#         if len(os.listdir(tmp_path)):\n",
    "#             print('Directory {} is not empty, skipping calculation.\\n'.format(tmp_path))\n",
    "        if 1:\n",
    "            #Add method that return interlayer outputs\n",
    "            new_model = HaveInterlayers(model.cuda(), loaders['train'])\n",
    "            #Environment that has method of load data to model and return interlayers\n",
    "            env = Evaluator(new_model, loaders)\n",
    "            iteration = 0\n",
    "            #min(n_features, N_samples) x n_features\n",
    "            B = []\n",
    "            max_iteration = max(config.svd_nsamples)/config.batch_size\n",
    "\n",
    "            for A in env.get_interlayer():\n",
    "                if iteration == 0:\n",
    "                    for j in range(len(A)):\n",
    "                        size_to_log = min(int(np.ceil(A[j].shape[1]/config.batch_size)*config.batch_size), \n",
    "                                          int(config.svd_nsamples[j]/config.batch_size)*config.batch_size)\n",
    "                        if size_to_log == 0:\n",
    "                            continue\n",
    "                        B.append(np.zeros((size_to_log, A[j].shape[1]), dtype=np.float32))\n",
    "                        B[j][:config.batch_size,:] = A[j]\n",
    "                        max_iteration = max(max_iteration, B[j].shape[0]/config.batch_size)\n",
    "                elif iteration >= max_iteration:\n",
    "                    break\n",
    "                else:\n",
    "                    for i in range(len(A)):\n",
    "                        if (config.batch_size*(iteration+1)) < min(B[i].shape):\n",
    "                            B[i][config.batch_size*iteration:config.batch_size*(iteration+1),:] = A[i]\n",
    "                iteration += 1\n",
    "\n",
    "            for i in range(len(B)):\n",
    "                h5file = tables.open_file(tmp_path+'/{}.h5'.format(i), mode='w')\n",
    "                root = h5file.root\n",
    "                h5file.create_array(root, 'B'+str(i), B[i])\n",
    "                h5file.close()  \n",
    "    if config.compute_svd:\n",
    "        dump_svd(config.dataset+'/'+config.model, config.storage_path+'interlayers/', max_r=config.svd_rank, k=50)\n",
    "else:\n",
    "    tmp_path = config.storage_path+'interlayers/'+config.dataset+'/'+config.model\n",
    "    if not os.path.exists(tmp_path+'/svd/'):\n",
    "        os.mkdir(tmp_path+'/svd/')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.validate_svd:\n",
    "    tmp_path = config.storage_path+'interlayers/'+config.dataset+'/'+config.model\n",
    "    for i in range(1,7):\n",
    "        g_t = tmp_path+'/{}.h5'.format(i)\n",
    "        filename = tmp_path+'/svd/SVD_after_block_{}.pickle'.format(i)\n",
    "        with open(filename, 'rb') as f:\n",
    "            (U,S,V) = pickle.load(f)\n",
    "            print(U.shape, V.shape)\n",
    "        with h5py.File(g_t, mode='r') as f:\n",
    "            for key,val in f.items():\n",
    "                X=torch.cuda.DoubleTensor(val[()])\n",
    "                print(X.shape)\n",
    "        print(X)\n",
    "        new_X = (U[:, ]@torch.diag(S)@V.t())\n",
    "        print(new_X)\n",
    "        new_X = torch.cuda.DoubleTensor(new_X.cuda())\n",
    "        print(torch.norm(X-new_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.plot_singular_values:    \n",
    "    tmp_path = config.storage_path+'interlayers/'+config.dataset+'/'+config.model\n",
    "    results_path = config.storage_path+'results/'+config.dataset+'/'+config.model\n",
    "\n",
    "    files = [f for f in os.listdir(tmp_path+'/svd/') if os.path.isfile(os.path.join(tmp_path+'/svd/', f))]\n",
    "    #save figures of sorted sigmas\n",
    "    if not os.path.exists(results_path+'/svd_imgs/'):\n",
    "        os.mkdir(results_path+'/svd_imgs/')\n",
    "    f = plt.figure(figsize=(1.35*3.25, 0.6*1.35*3.25))\n",
    "    legend = []\n",
    "    for i in range(len(files)):\n",
    "        filename = tmp_path+'/svd/SVD_after_block_{}.pickle'.format(i)\n",
    "        legend.append(i)\n",
    "        with open(filename, 'rb') as f:\n",
    "            (U,S,V) = pickle.load(f)\n",
    "            s = np.sort(S.cpu().numpy())[::-1][:min(V.shape[0], config.svd_maxval)-150]\n",
    "            s = s/s[0]\n",
    "        plt.semilogy(s)\n",
    "    plt.title(config.model.upper()+' on '+config.dataset.upper())\n",
    "    plt.ylabel('$\\sigma$')\n",
    "    plt.savefig(results_path+'/svd_imgs/'+config.model.upper()+'on'+config.dataset.upper()+'.pdf', bbox_inches='tight')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.calc_compressed:                    \n",
    "    W_shapes, block_indices = slice_blocks(model, loaders['val'], config.device)\n",
    "    tmp_path = config.storage_path+'interlayers/'+config.dataset+'/'+config.model\n",
    "    SVD_list = []\n",
    "    for idx, W in enumerate(W_shapes):\n",
    "        filename = tmp_path + '/svd/SVD_after_block_{}.pickle'.format(idx)\n",
    "        with open(filename, 'rb') as f:\n",
    "            U, S, V = pickle.load(f)\n",
    "        SVD_list += [(U.numpy(), S.numpy(), V.numpy())]\n",
    "    \n",
    "    if not os.path.exists(results_path+'/compression_results.txt'):\n",
    "        os.remove(results_path+'/compression_results.txt')\n",
    "    \n",
    "     \n",
    "    for compression_rate in config.compression_rate_list:\n",
    "        print(compression_rate)\n",
    "\n",
    "        compressed_model = FeedforwardCompressed(model, loaders['val'], compression_rate, 'cpu')\n",
    "        print('Fitting...')\n",
    "        compressed_model.fit(loaders['val'], SVD_list)\n",
    "\n",
    "        print('Creating student...')\n",
    "        student = Student(compressed_model, device = config.device)\n",
    "\n",
    "        student.to('cpu')\n",
    "        save_path = config.storage_path+'compressed/'+config.dataset+'/'+\\\n",
    "                    config.model+'/{}.pth'.format(compression_rate)\n",
    "        torch.save(student, save_path)\n",
    "        \n",
    "        results_path = config.storage_path+'results/'+config.dataset+'/'+config.model\n",
    "        \n",
    "        if config.eval_compressed:\n",
    "            print_info = {\n",
    "             'dataset': config.dataset,\n",
    "             'batch_size' : config.batch_size,\n",
    "             'arch': config.model,\n",
    "             'compression_rate': compression_rate,\n",
    "             'filename': results_path+'/compression_results.txt'  \n",
    "            }\n",
    "            validate(loaders['val'], student, print_info, device=config.device, is_svhn=(config.model=='svhn'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.calc_finetune:\n",
    "    tmp_path = config.storage_path+'compressed/'+config.dataset+'/'+config.model\n",
    "    results_path = config.storage_path+'results/'+config.dataset+'/'+config.model\n",
    "    if not os.path.exists(tmp_path+'/finetuned/'):\n",
    "        os.mkdir(tmp_path+'/finetuned/')\n",
    "\n",
    "    teacher_model = torch.load(tmp_path+'/{}.pth'.format(config.teacher), map_location='cpu')\n",
    "\n",
    "    for compression_rate in config.finetune_compression_rate_list:\n",
    "        best_prec1 = 0\n",
    "        precs = np.arange(10)         \n",
    "        print('Starting finetune model: {}'.format(compression_rate))\n",
    "        model_path = tmp_path+'/{}.pth'.format(compression_rate)\n",
    "        ft_model_path = tmp_path+'/finetuned/{}'.format(compression_rate)\n",
    "        ft_model = torch.load(model_path, map_location=config.device)\n",
    "\n",
    "        #Add Dropout before compressed layers\n",
    "        for i,m in enumerate(ft_model.features):\n",
    "            if 'Flatten' in str(m):\n",
    "                ft_model.features[i] = nn.Sequential(ft_model.features[i])\n",
    "                ft_model.features[i].add_module('drop', nn.Dropout())\n",
    "\n",
    "        optimizer = torch.optim.Adam(ft_model.parameters(), optim_parameters.lr0,\n",
    "            weight_decay=optim_parameters.weight_decay)\n",
    "\n",
    "        print('Optimizer: {}'.format(optimizer))\n",
    "        #Dir for finetuned model\n",
    "        if not os.path.exists(ft_model_path):\n",
    "            os.mkdir(ft_model_path)\n",
    "        #CSV with training results\n",
    "        with open(results_path+'/'+str(compression_rate)+'/finetune_results.csv', 'w') as f:\n",
    "            f.write('{},{},{},{},{},{},{},{}\\n'.format('epoch', 'epoch_time', 'train@1', 'train@5', 'train loss', 'val@1', 'val@5', 'val loss', ))\n",
    "\n",
    "        for epoch in range(optim_parameters.start_epoch, optim_parameters.epochs):\n",
    "\n",
    "            lr = adjust_learning_rate(optimizer, epoch, lr0=optim_parameters.lr0)\n",
    "            print('Epoch: [{}/{}]\\tlr: {}'.format(epoch, optim_parameters.epochs, lr))\n",
    "            # train for one epoch\n",
    "            end = time.time()\n",
    "            train(loaders['train'], ft_model, teacher_model, optimizer, epoch, kd_params, config.device, is_svhn=(config.model=='svhn'))\n",
    "            epoch_time = time.time()-end\n",
    "            # evaluate on validation set\n",
    "            print('Train:\\n')\n",
    "            train_top1, train_top5, train_losses = validate(loaders['train'], ft_model, device=config.device, is_svhn=(config.model=='svhn'))\n",
    "            print('Val:\\n')\n",
    "            top1, top5, losses = validate(loaders['val'], ft_model, device=config.device, is_svhn=(config.model=='svhn'))\n",
    "            prec1=top1.avg\n",
    "            # remember best prec@1 and save checkpoint\n",
    "            is_best = prec1 > best_prec1\n",
    "            best_prec1 = max(prec1, best_prec1)\n",
    "            precs[int(epoch%10)] = best_prec1\n",
    "\n",
    "            with open(results_path+'/'+str(compression_rate)+'/finetune_results.csv', 'a') as f:\n",
    "                f.write('{},{},{},{},{},{},{},{}\\n'.format(epoch + 1, epoch_time, train_top1.avg, train_top5.avg, train_losses.avg,\n",
    "                                               top1.avg, top5.avg, losses.avg))\n",
    "\n",
    "            save_checkpoint({\n",
    "                'epoch': epoch + 1,\n",
    "                'arch': name,\n",
    "                'state_dict': ft_model.state_dict(),\n",
    "                'best_prec1': best_prec1,\n",
    "                'optimizer' : optimizer.state_dict()\n",
    "            }, is_best, ft_model_path)\n",
    "            print (np.var(precs))\n",
    "            if np.var(precs) < config.ft_variance:\n",
    "                print('STABLE @ {}\\n'.format(epoch))\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.measure_speed:\n",
    "     \n",
    "    for compression_rate in config.compression_rate_list:\n",
    "        print(compression_rate)\n",
    "        results_path = config.storage_path+'results/'+config.dataset+'/'+config.model\n",
    "        save_path = config.storage_path+'compressed/'+config.dataset+'/'+\\\n",
    "                    config.model+'/{}.pth'.format(compression_rate)\n",
    "        student = torch.load(save_path)\n",
    "        student.to(config.speed_device)\n",
    "        student.eval()\n",
    "        speedloaders = dataloaders.get_loader(config.speed_batchsize, config.dataset, config.data_root, num_workers=4)\n",
    "        result_df = pd.DataFrame()\n",
    "        times_block = block_speed(student, speedloaders['val'], config.speed_device)\n",
    "        whole_timeit = timeit_speed(student, speedloaders['val'], config.speed_device)\n",
    "        result_df['Block'] = times_block\n",
    "        result_df['Whole'] = whole_timeit+[0]*(len(times_block)-1)\n",
    "        result_df.to_pickle(results_path+'/{}_{}.pickle'.format(compression_rate, device))\n",
    "        print('DONE: %d'%i)\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config.vis_time_accuracy:\n",
    "    def show_results(path):\n",
    "        with open(path, 'r') as f:\n",
    "            info = f.readlines()\n",
    "\n",
    "        columns = ['dataset', 'arch', 'compression_rate', 'batch_size',\n",
    "           'losses.avg', 'losses.get_var()', 'top1.avg', 'top5.avg',\n",
    "           'batch_time.avg', 'batch_time.get_var()', 'device']\n",
    "        info_data = pd.DataFrame([line.strip().split('\\t') for line in info], columns=columns)\n",
    "\n",
    "        for column in columns:\n",
    "            if column not in ['dataset', 'arch', 'compression_rate', 'device']:\n",
    "                info_data[column] = info_data[column].astype('float')\n",
    "\n",
    "        info_rate = []\n",
    "        for l in info_data['compression_rate']:\n",
    "            rates = list(filter(lambda el: el!='None', [el.strip() for el in l.strip('[,]').split(',')]))\n",
    "            rates = np.array(rates, dtype=float)\n",
    "\n",
    "            if len(rates) != 0:\n",
    "                info_rate.append([len(rates), rates[0]])\n",
    "            else:\n",
    "                info_rate.append([len(rates), 1])\n",
    "\n",
    "        info_data[['N_compressed_blocks', 'rate']] =  pd.DataFrame(info_rate)\n",
    "        return info_data  \n",
    "    \n",
    "    \n",
    "    def get_acc_speed(config, compression_rate):\n",
    "        results_path = config.storage_path+'results/'+config.dataset+'/'+config.model       \n",
    "        finetune_path = config.storage_path+'compressed/'+config.dataset+'/'+config.model+\\\n",
    "                        '/finetuned/{}/'.format(compression_rate)\n",
    "        best_epoch = torch.load(finetune_path+'model_best.pth.tar')['epoch']\n",
    "        acc_df = pd.read_csv(results_path + '/' + str(compression_rate)+'/finetune_results.csv')\n",
    "    \n",
    "        if os.path.exists(results_path + '/{}_{}.pickle'.format(compression_rate, config.speed_device)):\n",
    "            with open(results_path + '/{}_{}.pickle'.format(compression_rate, config.speed_device), 'rb') as f:\n",
    "                df_column = pickle.load(f)\n",
    "        \n",
    "        total_time = df_column['Whole'][0]\n",
    "\n",
    "        info_data = show_results(results_path + '/compression_results.txt')\n",
    "        info_data_dataset = info_data[info_data['dataset']==config.dataset]\n",
    "        row = info_data_dataset[info_data_dataset['compression_rate']==str(compression_rate)]\n",
    "        print(acc_df)\n",
    "        return total_time.average, acc_df.loc[best_epoch-1, 'val@1'], acc_df.loc[best_epoch-1, 'val@5'], row['top1.avg'], row['top5.avg']\n",
    "    \n",
    "\n",
    "    rc('font',**{'family':'serif','sans-serif':['Computer Modern Roman']})\n",
    "    rcParams.update({'font.size': 10})\n",
    "    f = plt.figure(figsize=(1.35*3.25, 1.35*0.6*3.25))\n",
    "    #Teacher first\n",
    "    t, prec_ft, prec_ft5, prec, prec5 = get_acc_speed(config, config.teacher)\n",
    "    original_time = t\n",
    "    speed = 1.\n",
    "    plt.scatter(1, prec_ft, marker='o', c=(0.1, 0.2, 0.5))\n",
    "    plt.axvline(x=1, linestyle='--', color=(0.1, 0.2, 0.5), lw=0.5)\n",
    "    plt.axhline(y=prec_ft, linestyle='--', color=(0.1, 0.2, 0.5), lw=0.5)\n",
    "    prec_ft_bl = prec_ft\n",
    "\n",
    "    #Lists for figure\n",
    "    color_list = ['{},{},{}'.format(0.1, 0.2, 0.5)]   \n",
    "    accuracy_list_before = [prec]\n",
    "    accuracy_list_after = [prec_ft]\n",
    "    accuracy_list_before5 = [prec5]\n",
    "    accuracy_list_after5 = [prec_ft5]\n",
    "    speed_up_list = [speed]\n",
    "\n",
    "    for i, compression_rate in enumerate(config.finetune_compression_rate_list):\n",
    "        t, prec_ft, prec_ft5, prec, prec5 = get_acc_speed(config, compression_rate)\n",
    "        speed = original_time/t\n",
    "        plt.scatter(speed, prec_ft, marker='o', c=(0.3*i%1, 0.2*i%1, 0.1*i%1))\n",
    "        plt.scatter(speed, prec, marker='^', c=(0.3*i%1, 0.2*i%1, 0.1*i%1))\n",
    "\n",
    "        color_list.append('{},{},{}'.format(0.3*i%1, 0.2*i%1, 0.1*i%1))    \n",
    "        accuracy_list_before.append(prec)\n",
    "        accuracy_list_after.append(prec_ft)\n",
    "        accuracy_list_before5.append(prec5)\n",
    "        accuracy_list_after5.append(prec_ft5)\n",
    "        speed_up_list.append(speed)\n",
    "\n",
    "    plt.xlabel('Speed up')\n",
    "    plt.ylabel('Accuracy, %')\n",
    "    plt.xlim([0.9, 3.1])\n",
    "    plt.ylim(ylims[dataset])\n",
    "    plt.title('Speed vs Accuracy for '+'{}'.format(dataset).upper())\n",
    "    plt.savefig(config.storage_path+'results/'+config.dataset+'/'+config.model+'/speed_accuracy_{}.pdf'.format(dataset), bbox_inches='tight')\n",
    "    # plt.legend(names, loc=10)\n",
    "    plt.show()\n",
    "\n",
    "    table = pd.DataFrame()\n",
    "    beauty_names = ['Teacher'] + ['{:.1f}x {:d} blocks'.format(1/x.max(), int(sum(x!=None))) \n",
    "                    for x in config.finetune_compression_rate_list]\n",
    "\n",
    "    table['Name'] = beauty_names\n",
    "    table['Color'] = color_list\n",
    "    table['Prec@1 \\\\newline before FT'] = accuracy_list_before\n",
    "    table['Prec@5 \\\\newline before FT'] = accuracy_list_before5\n",
    "\n",
    "    table['Prec@1 \\\\newline after FT'] = accuracy_list_after\n",
    "    table['Prec@5 \\\\newline after FT'] = accuracy_list_after5\n",
    "\n",
    "#     table['Speed up conv part'] = speed_up_list\n",
    "    table['Speed up'] = speed_up_list\n",
    "    table = table.sort_values(by=['Speed up'])\n",
    "    print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
